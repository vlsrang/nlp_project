{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.probability import FreqDist\n",
        "import nltk\n",
        "\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "!apt-get update\n",
        "!apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
        "!pip3 install JPype1-py3\n",
        "!pip3 install konlpy"
      ],
      "metadata": {
        "id": "0Wmxz0VmLfCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "\n",
        "okt = Okt()\n",
        "# 한국 시간대로 설정하기\n",
        "korea_tz = pytz.timezone('Asia/Seoul')\n",
        "\n",
        "# 현재 시간을 가져오기\n",
        "current_datetime = datetime.now(korea_tz)\n",
        "\n",
        "# 한국 시간으로 현재 날짜와 시간을 출력하기\n",
        "print(\"한국 시간 기준 현재 날짜와 시간:\", current_datetime)\n",
        "\n",
        "# 첫 번째 URL에서 기사 제목 가져오기\n",
        "def get_article_title(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    title = soup.select_one('#content > div > div.content > div > div.news_headline > h4')\n",
        "    return title.get_text().strip() if title else None\n",
        "\n",
        "# 두 번째 URL에서 기사 제목들 가져오기\n",
        "def get_article_titles(url):\n",
        "    titles = []\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    articles = soup.select('a.title > span')\n",
        "    for article in articles:\n",
        "        title_text = article.get_text(strip=True)\n",
        "        titles.append(title_text)\n",
        "    return titles\n",
        "\n",
        "# 기사 내용 불러오기\n",
        "def get_article_text(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    article_text = ''\n",
        "    article = soup.find('div', {'class': 'news_end'})\n",
        "    if article:\n",
        "        for unwanted_1 in article.find_all('em', {'class': 'img_desc'}):\n",
        "            unwanted_1.extract()\n",
        "\n",
        "        for unwanted_2 in article.find_all(string=re.compile(r'\\[.*?\\]')):\n",
        "            unwanted_2.extract()\n",
        "\n",
        "        unwanted_3 = article.find_all(['div', 'p'], {'class': ['source', 'byline', 'reporter_area', 'copyright', 'categorize', 'promotion']})\n",
        "        for unwanted in unwanted_3:\n",
        "            unwanted.extract()\n",
        "\n",
        "        article_text = article.get_text(separator='\\n', strip=True)\n",
        "\n",
        "    return article_text\n",
        "\n",
        "# TF-IDF 벡터화 및 유사도 계산하기\n",
        "def calculate_tfidf_similarity(title1, titles2):\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform([title1] + titles2)\n",
        "    similarity_matrix = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])\n",
        "    return similarity_matrix\n",
        "\n",
        "# 유사한 제목들을 출력하기\n",
        "def find_similar_titles(similarity_matrix, title1, titles2, threshold=0.03):\n",
        "    similar_titles = []\n",
        "    for i, similarity_score in enumerate(similarity_matrix.flatten()):\n",
        "        if similarity_score > threshold:\n",
        "            similar_titles.append((title1, titles2[i], similarity_score))\n",
        "    return similar_titles\n",
        "\n",
        "# 두 개의 URL에서 기사 제목을 가져오기\n",
        "url1 = input(\"원하는 네이버스포츠 기사를 입력하세요: \")\n",
        "url2 = \"https://sports.news.naver.com/kbaseball/news/index?isphoto=N\"\n",
        "title1 = get_article_title(url1)\n",
        "titles2 = get_article_titles(url2)\n",
        "for t2 in titles2:\n",
        "  if title1 == t2:\n",
        "    titles2.remove(t2)\n",
        "print(f\"기사 제목: {title1}\")\n",
        "\n",
        "if title1 and titles2:\n",
        "    # TF-IDF 유사도 계산하기\n",
        "    similarity_matrix = calculate_tfidf_similarity(title1, titles2)\n",
        "\n",
        "    # 유사한 제목들을 출력하기\n",
        "    similar_titles = find_similar_titles(similarity_matrix, title1, titles2)\n",
        "    if similar_titles:\n",
        "        print(\"유사한 제목들:\")\n",
        "        for title_pair in set(similar_titles):\n",
        "            print(f\"URL 기사 제목: {title_pair[0]}\")\n",
        "            print(f\"기사 2 제목: {title_pair[1]} (유사도: {title_pair[2]:.4f})\")\n",
        "            print(\"----------\")\n",
        "    else:\n",
        "        print(\"유사한 제목이 없습니다.\")\n",
        "else:\n",
        "    print(\"기사 제목을 가져올 수 없습니다.\")\n",
        "\n",
        "# 기사 요약하기\n",
        "def summarize_article(text):\n",
        "    tokens = okt.nouns(text)\n",
        "    word_freq = Counter(tokens)\n",
        "\n",
        "    most_freq_words = [word[0] for word in word_freq.most_common(3)]\n",
        "\n",
        "    sentences = text.split('.')\n",
        "\n",
        "    summary_sentences = []\n",
        "    for sentence in sentences:\n",
        "        for word in most_freq_words:\n",
        "            if word in sentence and sentence.strip() not in summary_sentences and len(summary_sentences) < 3:\n",
        "                summary_sentences.append(sentence.strip())\n",
        "\n",
        "    summary = '. '.join(summary_sentences)\n",
        "    return summary\n",
        "\n",
        "# 기사 내용을 가져오기\n",
        "article_text = get_article_text(url1)\n",
        "\n",
        "# 기사 내용 요약\n",
        "summary = summarize_article(article_text)\n",
        "\n",
        "print(\"기사 요약:\\n\", summary)\n",
        "\n"
      ],
      "metadata": {
        "id": "-VvotNd9xYIU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fceff14-c84a-4696-f7cc-a00625d59eea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "한국 시간 기준 현재 날짜와 시간: 2023-12-17 10:43:16.485791+09:00\n",
            "원하는 네이버스포츠 기사를 입력하세요: https://sports.news.naver.com/news?oid=003&aid=0012270766\n",
            "기사 제목: SSG, 에레디아·엘리아스와 재계약…2024시즌 외인 구성 완료\n",
            "유사한 제목들:\n",
            "URL 기사 제목: SSG, 에레디아·엘리아스와 재계약…2024시즌 외인 구성 완료\n",
            "기사 2 제목: \"재계약 보험? 데려올 선수가 없어요\" 구단들 역대급 구인난, 어디서부터 꼬였을까? (유사도: 0.0766)\n",
            "----------\n",
            "URL 기사 제목: SSG, 에레디아·엘리아스와 재계약…2024시즌 외인 구성 완료\n",
            "기사 2 제목: 난세에 중책…김재현 SSG 단장 \"진정성 갖고 팬들에게 다가가겠다\" (유사도: 0.0972)\n",
            "----------\n",
            "URL 기사 제목: SSG, 에레디아·엘리아스와 재계약…2024시즌 외인 구성 완료\n",
            "기사 2 제목: 돌아온 MVP+재계약 성공한 좌승사자, LG 왕조 건립 막아서나 (유사도: 0.0826)\n",
            "----------\n",
            "기사 요약:\n",
            " 지난달 새 외국인 투수 더거 영입\n",
            "SSG는 외국인 타자 에레디아와 총액 150만달러(계약금 15만달러·연봉 115만달러·옵션 20만달러)에, 외국인 투수 엘리아스와 총액 100만달러(계약금 10만달러·연봉 65만달러·옵션 25만달러)에 각각 재계약을 체결했다고 17일 밝혔다. 지난달 말 새 외국인 투수 로버트 더거와 총액 90만 달러(계약금 10만달러·연봉 65만달러·옵션 15만달러)에 계약한 SSG는 이로써 2024시즌 외국인 선수 구성을 모두 마쳤다. 지난해 12월 신규 외국인 선수 상한선인 100만달러를 꽉 채워 계약했던 엘리아스는 50만달러 오른 금액에 사인했다\n"
          ]
        }
      ]
    }
  ]
}